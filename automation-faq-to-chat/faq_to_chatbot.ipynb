{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e5fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install BackTranslation\n",
    "# ! pip install benepar\n",
    "# ! pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git\n",
    "# ! pip install expertai-nlapi==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8c1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d23987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7c966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e515fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\AH20780\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "C:\\ProgramData\\Anaconda3\\envs\\anuj_research\\lib\\site-packages\\benepar\\spacy_plugin.py:7: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from BackTranslation import BackTranslation\n",
    "import spacy\n",
    "import benepar\n",
    "benepar.download('benepar_en3')\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# import tensorflow_hub as hub \n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "import warnings\n",
    "from rake_nltk import Rake\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d7ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AH20780\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bdd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils():\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self): \n",
    "        pass \n",
    "    \n",
    "   \n",
    "    '''\n",
    "    Translate other language to english for a given sentance\n",
    "    @in_word : Input text to be translated from detected text language to english\n",
    "    @return - Translated text. In case language is not getting detected, method returns input text\n",
    "    '''\n",
    "    def translate_to_en(self, in_word):\n",
    "        try:\n",
    "            translator = Translator()\n",
    "            t = translator.detect(in_word)\n",
    "            if(t.lang==\"tl\" ): # Filipino\n",
    "                translated = translator.translate(in_word, src='tl', dest='en')\n",
    "                return translated.text\n",
    "            if(t.lang==\"hi\" ): # Hindi\n",
    "                translated = translator.translate(in_word, src='hi', dest='en')\n",
    "                return translated.text\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in method translate_to_en -->\"+str(e))\n",
    "\n",
    "        return in_word;    \n",
    "       \n",
    "    # Filipino\n",
    "    def translate_to_tl(self, in_word):\n",
    "        try:\n",
    "            translator = Translator()\n",
    "            t = translator.detect(in_word)\n",
    "            if(t.lang==\"en\" ):\n",
    "                translated = translator.translate(in_word, src='en', dest='tl')\n",
    "                return translated.text\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in method translate_to_tl -->\"+str(e))\n",
    "           \n",
    "        return in_word; \n",
    "    \n",
    "    # Hindi\n",
    "    def translate_to_hi(self, in_word):\n",
    "        try:\n",
    "            translator = Translator()\n",
    "            t = translator.detect(in_word)\n",
    "            if(t.lang==\"en\" ):\n",
    "                translated = translator.translate(in_word, src='en', dest='hi')\n",
    "                return translated.text\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error occured in method translate_to_hi -->\"+str(e))\n",
    "            \n",
    "        return in_word; \n",
    "    \n",
    "    # mid is the language param\n",
    "    def back_translation(self, in_word, mid):\n",
    "        bt = in_word\n",
    "        if( mid=='de'):\n",
    "            bt= self.translate_to_tl(in_word)\n",
    "        elif (mid=='fr'):\n",
    "            bt= self.translate_to_hi(in_word) \n",
    "        return self.translate_to_en(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77f2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the translation utility\n",
    "# utl = Utils()\n",
    "# et = \"Facebook chief executive officer Mark Zuckerberg on Thursday announced that the company he founded is rebranding itself as Meta.\"\n",
    "# et = utl.back_translation(et, mid='fr')\n",
    "# print(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e23689",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_NAME = \"ACUBED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0a973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backtranslation(txt):\n",
    "    bt_sents=[]\n",
    "    \n",
    "    trans = BackTranslation(url=[\n",
    "          'translate.google.com'\n",
    "        ], proxies={'http': '127.0.0.1:1234', 'http://host.name': '127.0.0.1:4012'})\n",
    "    result = trans.translate(txt, src='en')\n",
    "    bt_sents.append(result.result_text)\n",
    "    \n",
    "    utls= Utils()\n",
    "    et = utls.back_translation(txt,'hi')\n",
    "    bt_sents.append(et)\n",
    "    et = utls.back_translation(txt,'tl')\n",
    "    bt_sents.append(et)\n",
    "    return bt_sents\n",
    "    \n",
    "\n",
    "def nv_phrase_backtranslation(txt):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    if spacy.__version__.startswith('2'):\n",
    "        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "    else:\n",
    "        nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "    doc = nlp(txt)\n",
    "    sent = list(doc.sents)[0]\n",
    "    print(f'BT String: {sent._.parse_string}')\n",
    "    # (S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
    "    print(f'BT String: {sent._.labels}')\n",
    "    # ('S',)\n",
    "    print(f'BT String: {list(sent._.children)[0]}')\n",
    "    # The time for action\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    words = words.lower()\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in words:\n",
    "        if ele in punc:\n",
    "            words = words.replace(ele, \"\")\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        \n",
    "        if num_replaced >= n: # only replace up to n words\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def synonym_generator(words):\n",
    "    words = words.lower()\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in words:\n",
    "        if ele in punc:\n",
    "            words = words.replace(ele, \"\")\n",
    "    \n",
    "    words = words.split()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        print(f'synonyms for \"{random_word}\": {synonyms}') \n",
    "\n",
    "\n",
    "def generate_paraphrase(txt): \n",
    "    retVal = []\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    ''' \n",
    "    uncomment to get reproducable paraphrase generations\n",
    "    def random_state(seed):\n",
    "      torch.manual_seed(seed)\n",
    "      if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    random_state(1234)\n",
    "    '''\n",
    "        \n",
    "    #Init models (make sure you init ONLY once if you integrate this to your code)\n",
    "    parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "    \n",
    "    print(\"With default knobs:\")\n",
    "    para_phrases = parrot.augment(input_phrase=txt, use_gpu=False)\n",
    "    for para_phrase in para_phrases:\n",
    "        print(para_phrase)\n",
    "        retVal.append(para_phrase[0])\n",
    "    \n",
    "    print(\"\\nWith Diversity Ranker as Levenshtein:\")\n",
    "    para_phrases = parrot.augment(input_phrase=txt, use_gpu=False, diversity_ranker=\"levenshtein\")\n",
    "    for para_phrase in para_phrases:\n",
    "        print(para_phrase)\n",
    "        retVal.append(para_phrase[0])\n",
    "    \n",
    "    print(\"\\nWith Do_diverse set:\")\n",
    "    para_phrases = parrot.augment(input_phrase=txt, use_gpu=False, do_diverse=True)\n",
    "    for para_phrase in para_phrases:\n",
    "        print(para_phrase)\n",
    "        retVal.append(para_phrase[0])\n",
    "        \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd51711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(txt):\n",
    "    example_sent =txt\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    return  filtered_sentence\n",
    "\n",
    "\n",
    "def extract_theme_words(txt):\n",
    "    retVal=[]\n",
    "    rake_nltk_var = Rake()\n",
    "    rake_nltk_var.extract_keywords_from_text(txt)\n",
    "    keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    print(f'RAKE extracted keywords for understanding intention: {keyword_extracted}')\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Single keywords of key words are being appended\n",
    "    words_appended = []\n",
    "    for key_words in keyword_extracted:\n",
    "        tokens = tokenizer.tokenize(key_words)\n",
    "        if (len(tokens) == 1):\n",
    "            words_appended.append(tokens[0])\n",
    "        else :\n",
    "            retVal.append(key_words)\n",
    "    \n",
    "    if (words_appended):\n",
    "        if ((len (words_appended) ==1) and retVal):\n",
    "            # Single word appended contains one word, and its added to first keyword extract string\n",
    "            retVal[0] = retVal[0] + ' '+ ' '.join(words_appended)\n",
    "        else:\n",
    "            retVal.append(' '.join(words_appended))    \n",
    "        \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f7668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out top 2 similar sentence for the paraphrase_pool\n",
    "def filter_duplication(txt, utterances):\n",
    "\n",
    "    # get cosine similairty matrix\n",
    "    def cos_sim(input_vectors):\n",
    "        similarity = cosine_similarity(input_vectors)\n",
    "        return similarity\n",
    "    \n",
    " \n",
    "    # get topN similar sentences\n",
    "    def get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n",
    "        # find the index of sentence in list\n",
    "        index = sentence_list.index(sentence)\n",
    "        # get the corresponding row in similarity matrix\n",
    "        similarity_row = np.array(similarity_matrix[index, :])\n",
    "        # get the indices of top similar\n",
    "        indices = similarity_row.argsort()[-topN:][::-1]\n",
    "        return [sentence_list[i] for i in indices]\n",
    "        \n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "      \n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    embed = hub.Module(module_url)\n",
    "   \n",
    "    # Reduce logging output.\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "   \n",
    "    sentences_list = utterances\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentences_embeddings = session.run(embed(sentences_list))\n",
    "    \n",
    "    similarity_matrix = cos_sim(np.array(sentences_embeddings))\n",
    "    \n",
    "    sentence = txt\n",
    "    top_similar = get_top_similar(sentence, sentences_list, similarity_matrix, 3)\n",
    "   \n",
    "    # printing the list using loop \n",
    "    for x in range(len(top_similar)): \n",
    "        print(top_similar[x])\n",
    "    \n",
    "    #get the index of a given item    \n",
    "    indx = utterances.index(txt)\n",
    "    print(\"indx=\",indx)\n",
    "    #remove item from list\n",
    "    utterances.pop(indx)\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a617b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_utterance (questionTxt): \n",
    "    retVal=[]\n",
    "    print(\"Question Text-->\",questionTxt)\n",
    "    \n",
    "    #add org text in the pool\n",
    "    retVal.append(questionTxt)\n",
    "    \n",
    "    nv_phrase_backtranslation(questionTxt)\n",
    "    \n",
    "    a=full_backtranslation(questionTxt)\n",
    "    print(\"Backtranslation Text-->\",a)\n",
    "    retVal.extend(a)\n",
    "    \n",
    "    a=extract_theme_words(questionTxt)\n",
    "    print(\"Extracted Theme Words-->\",a)\n",
    "    retVal.extend(a)\n",
    "     \n",
    "    a = synonym_replacement(questionTxt, 2)\n",
    "    print(\"Synonyms-->\",a)\n",
    "    retVal.extend([a])\n",
    "    \n",
    "    synonym_generator(questionTxt)\n",
    "    \n",
    "    print(\"\\nParrot Paraphrase with Similarity Index-->\")\n",
    "    a=generate_paraphrase(questionTxt)\n",
    "    print(\"\\nParrot Paraphrase Extracted List-->\",a)\n",
    "    retVal.extend(a)\n",
    "    \n",
    "    \n",
    "    #convert to lower case list followed by unique list\n",
    "    retVal = [each_string.lower() for each_string in retVal]\n",
    "    retVal = list(set(retVal))\n",
    "    \n",
    "    print(\"\\nUtterances List-->\",retVal)\n",
    "    \n",
    "    # Duplication removal - use only if required\n",
    "#     retVal = filter_duplication(questionTxt.lower(), retVal)\n",
    "#     print(\"Duplication Filtered Utterances-->\",retVal)\n",
    "     \n",
    "    \n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef4fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent (questionTxt):\n",
    "    \n",
    "    questionTxt= questionTxt.replace(CHATBOT_NAME, \"chatbot\") # replace chatbot name as \"chatbot\"\n",
    "    print(f'Question text--> {questionTxt}')\n",
    "    intent= []\n",
    "    \n",
    "    with open('.env') as f:\n",
    "        lines = f.readlines()\n",
    "        username = lines[0].strip()\n",
    "        password = lines[1].strip()\n",
    "    os.environ[\"EAI_USERNAME\"] = username\n",
    "    os.environ[\"EAI_PASSWORD\"] = password\n",
    "    \n",
    "    from expertai.nlapi.cloud.client import ExpertAiClient\n",
    "    client = ExpertAiClient()\n",
    "    language= 'en'\n",
    "    document = client.specific_resource_analysis(\n",
    "        body={\"document\": {\"text\": questionTxt}}, \n",
    "        params={'language': language, 'resource': 'relevants'})\n",
    "    \n",
    " \n",
    "    print('')  \n",
    "    phrases={}\n",
    "    for main_phrase in document.main_phrases:\n",
    "        print (f'{main_phrase.value:{20}} {main_phrase.score:{5}}')\n",
    "        phrases[main_phrase.value]=main_phrase.score\n",
    "    \n",
    "    print(\"Phrases==>\",phrases)\n",
    "    if phrases:\n",
    "        max_key = max(phrases, key=phrases.get)\n",
    "        intent.extend(max_key.split())\n",
    "              \n",
    "    print('')    \n",
    "    concepts={}\n",
    "    for main_concept in document.main_syncons:\n",
    "        print (f'{main_concept.lemma:{20}} {main_concept.score:{5}}')\n",
    "        concepts[main_concept.lemma]=main_concept.score\n",
    "     \n",
    "    print(\"Concepts==>\",concepts)\n",
    "    if concepts:\n",
    "        max_key = max(concepts, key=concepts.get)\n",
    "        intent.extend(max_key.split())\n",
    "\n",
    "    print('')      \n",
    "    lemmas={}\n",
    "    for main_lemma in document.main_lemmas:\n",
    "        print (f'{main_lemma.value:{20}} {main_lemma.score:{5}}')\n",
    "        lemmas[main_lemma.value]=main_lemma.score\n",
    "    \n",
    "    print(\"Lemmas==>\",lemmas)\n",
    "    if lemmas:\n",
    "        max_key = max(lemmas, key=lemmas.get)\n",
    "        intent.extend(max_key.split())\n",
    "\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    print(f'Intent raw concepts: {intent}')\n",
    "    intent_max_word=10\n",
    "    return \"_\".join(list(dict.fromkeys(intent[:intent_max_word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd6d2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faq_csv_modinfire(csv_file_path):\n",
    "        csvfile = open(csv_file_path,errors=\"backslashreplace\")\n",
    "        csvfileDf = pd.read_csv(csvfile, sep=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvfileDf = csvfileDf.fillna('')\n",
    "        csvfileDf.columns = csvfileDf.columns.str.strip()\n",
    "        \n",
    "        #Create records from csvDF file\n",
    "        for index, row in csvfileDf.iterrows():\n",
    "            question= row['question']\n",
    "            \n",
    "            print(\"-\"*100)\n",
    "            print(\"\\n\")\n",
    "            print(\"Question-->\",question)\n",
    "        \n",
    "            print(\"\\nUTTERANCE\")\n",
    "            utters_with_pipe=\"|\".join(question_to_utterance(question))\n",
    "            print(\"\\nDone!  These are the final Utterances-->\\n\",utters_with_pipe)\n",
    "            \n",
    "            print(\"\\nINTENT AND ENTITY\")\n",
    "            intent_txt= get_intent(question)\n",
    "            print(\"Done! Intent is -->\", intent_txt)\n",
    "            print(\"-\"*100)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            # updating the column value/data\n",
    "            csvfileDf.loc[index, 'utterances'] = utters_with_pipe\n",
    "            csvfileDf.loc[index, 'intent'] = intent_txt\n",
    "        \n",
    "        # writing into the file\n",
    "        csvfileDf.to_csv(csv_file_path.replace(\".csv\", \"_output.csv\"), index=False)\n",
    "        print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c54203ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\anuj_research\\lib\\site-packages\\torch\\distributions\\distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "FAQ_CSV_INTENT_FILE = 'faqs_raw_input.csv'\n",
    "faq_csv_modinfire(FAQ_CSV_INTENT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a1e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anuj_research",
   "language": "python",
   "name": "anuj_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
